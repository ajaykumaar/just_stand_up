{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3507dd41",
   "metadata": {},
   "source": [
    "# Just Stand Up\n",
    "Train a humanoid to stand up using reinforcement learning.\n",
    "\n",
    "This notebook sets up the environment and tools, but **you'll need to implement the key logic**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3391d8",
   "metadata": {},
   "source": [
    "## 📦 Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb45e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dm_control\n",
    "!pip install stable-baselines3\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b2551",
   "metadata": {},
   "source": [
    "## 🧠 Load the Humanoid environment (from dm_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import suite\n",
    "\n",
    "# Load humanoid standing task\n",
    "env = suite.load(domain_name=\"humanoid\", task_name=\"stand\")\n",
    "\n",
    "# Explore observation space\n",
    "obs_spec = env.observation_spec()\n",
    "print(\"Observation spec:\", obs_spec)\n",
    "\n",
    "# TODO: Wrap this env to make it compatible with stable-baselines3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2d8e6",
   "metadata": {},
   "source": [
    "## ⚙️ Define your Gym wrapper\n",
    "_You'll need to convert `dm_control` into a Gym-compatible environment._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744adeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a wrapper that exposes reset(), step(), observation_space, action_space\n",
    "# You may want to flatten the observation dict into a single np.ndarray\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class DMCWrapper(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = suite.load(domain_name=\"humanoid\", task_name=\"stand\")\n",
    "        self.observation_space = ...  # TODO\n",
    "        self.action_space = ...       # TODO\n",
    "\n",
    "    def reset(self):\n",
    "        ts = self.env.reset()\n",
    "        return self._flatten_obs(ts.observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        ts = self.env.step(action)\n",
    "        obs = self._flatten_obs(ts.observation)\n",
    "        reward = ts.reward or 0.0\n",
    "        done = ts.last()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def _flatten_obs(self, obs):\n",
    "        return np.concatenate([v.ravel() for v in obs.values()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87977e",
   "metadata": {},
   "source": [
    "## 🤖 Train with PPO (optional starter agent in src/agents/ppo_agent.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import your wrapped env and train with PPO\n",
    "# from stable_baselines3 import PPO\n",
    "# model = PPO('MlpPolicy', your_wrapped_env, verbose=1)\n",
    "# model.learn(total_timesteps=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53365065",
   "metadata": {},
   "source": [
    "## ✅ Evaluate your trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae355fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load model, run inference loop, visualize behavior (if desired)\n",
    "# obs = env.reset()\n",
    "# for _ in range(1000):\n",
    "#     action, _ = model.predict(obs)\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     if done:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d549c48",
   "metadata": {},
   "source": [
    "## 🧪 PPO Training on DummyEnv\n",
    "Here's a working example of PPO training on a simple dummy environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "class DummyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DummyEnv, self).__init__()\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(10,), dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.observation_space.sample()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs = self.observation_space.sample()\n",
    "        reward = np.random.rand()\n",
    "        done = np.random.rand() > 0.95\n",
    "        info = {}\n",
    "        return obs, reward, done, info\n",
    "\n",
    "env = DummyEnv()\n",
    "check_env(env)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "print(\"✅ PPO training on DummyEnv completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
